{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.tsmixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from neuralforecast.losses.pytorch import MAE\n",
    "from neuralforecast.common._base_multivariate import BaseMultivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TSMixer(BaseMultivariate):\n",
    "    \"\"\" TSMixer\n",
    "\n",
    "    TODO documentation here\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `h`: int, Forecast horizon. <br>\n",
    "    `input_size`: int, autorregresive inputs size, y=[1,2,3,4] input_size=2 -> y_[t-2:t]=[1,2].<br>\n",
    "    `n_series`: int, number of time-series.<br>\n",
    "    `stat_exog_list`: str list, static exogenous columns.<br>\n",
    "    `hist_exog_list`: str list, historic exogenous columns.<br>\n",
    "    `futr_exog_list`: str list, future exogenous columns.<br>\n",
    "    `n_stacks`: int=2, number of stacks in the model.<br>\n",
    "    `multi_layer`: int=5, multiplier for FC hidden size on StemGNN blocks.<br>\n",
    "    `dropout_rate`: float=0.5, dropout rate.<br>\n",
    "    `leaky_rate`: float=0.2, alpha for LeakyReLU layer on Latent Correlation layer.<br>\n",
    "    `loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
    "    `valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
    "    `max_steps`: int=1000, maximum number of training steps.<br>\n",
    "    `learning_rate`: float=1e-3, Learning rate between (0, 1).<br>\n",
    "    `num_lr_decays`: int=-1, Number of learning rate decays, evenly distributed across max_steps.<br>\n",
    "    `early_stop_patience_steps`: int=-1, Number of validation iterations before early stopping.<br>\n",
    "    `val_check_steps`: int=100, Number of training steps between every validation loss check.<br>\n",
    "    `batch_size`: int, number of windows in each batch.<br>\n",
    "    `step_size`: int=1, step size between each window of temporal data.<br>\n",
    "    `scaler_type`: str='robust', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n",
    "    `random_seed`: int, random_seed for pytorch initializer and numpy generators.<br>\n",
    "    `num_workers_loader`: int=os.cpu_count(), workers to be used by `TimeSeriesDataLoader`.<br>\n",
    "    `drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n",
    "    `alias`: str, optional,  Custom name of the model.<br>\n",
    "    `**trainer_kwargs`: int,  keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>    \n",
    "    \"\"\"\n",
    "    # Class attributes\n",
    "    SAMPLING_TYPE = 'multivariate'\n",
    "    \n",
    "    def __init__(self,\n",
    "                 h,\n",
    "                 input_size,\n",
    "                 n_series,\n",
    "                 futr_exog_list = None,\n",
    "                 hist_exog_list = None,\n",
    "                 stat_exog_list = None,\n",
    "                 n_stacks = 2,\n",
    "                 multi_layer: int = 5,\n",
    "                 dropout_rate: float = 0.5,\n",
    "                 leaky_rate: float = 0.2,\n",
    "                 loss = MAE(),\n",
    "                 valid_loss = None,\n",
    "                 max_steps: int = 1000,\n",
    "                 learning_rate: float = 1e-3,\n",
    "                 num_lr_decays: int = 3,\n",
    "                 early_stop_patience_steps: int =-1,\n",
    "                 val_check_steps: int = 100,\n",
    "                 batch_size: int = 32,\n",
    "                 step_size: int = 1,\n",
    "                 scaler_type: str = 'robust',\n",
    "                 random_seed: int = 1,\n",
    "                 num_workers_loader = 0,\n",
    "                 drop_last_loader = False,\n",
    "                 **trainer_kwargs):\n",
    "\n",
    "        # Inherit BaseMultivariate class\n",
    "        super(TSMixer, self).__init__(h=h,\n",
    "                                      input_size=input_size,\n",
    "                                      n_series=n_series,\n",
    "                                      futr_exog_list=futr_exog_list,\n",
    "                                      hist_exog_list=hist_exog_list,\n",
    "                                      stat_exog_list=stat_exog_list,                                    \n",
    "                                      loss=loss,\n",
    "                                      valid_loss=valid_loss,\n",
    "                                      max_steps=max_steps,\n",
    "                                      learning_rate=learning_rate,\n",
    "                                      num_lr_decays=num_lr_decays,\n",
    "                                      early_stop_patience_steps=early_stop_patience_steps,\n",
    "                                      val_check_steps=val_check_steps,\n",
    "                                      batch_size=batch_size,\n",
    "                                      step_size=step_size,\n",
    "                                      scaler_type=scaler_type,\n",
    "                                      num_workers_loader=num_workers_loader,\n",
    "                                      drop_last_loader=drop_last_loader,\n",
    "                                      random_seed=random_seed,\n",
    "                                      **trainer_kwargs)\n",
    "\n",
    "\n",
    "    def res_block(self, inputs, batch_size, input_size, num_channels):\n",
    "        self.batch_norm = nn.BatchNorm2d(num_features=num_channels)\n",
    "        self.linear = nn.Linear(in_features=input_size, out_features=input_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "        # time block\n",
    "        x = nn.BatchNorm2d(num_features=num_channels)\n",
    "        x = torch.transpose(x, 3, 2)\n",
    "        x = nn.Linear(in_features=num_channels, out_features=num_channels)(x)\n",
    "        x = nn.ReLU(x)\n",
    "        x = nn.Dropout(0.1)(x)\n",
    "        res = inputs + x\n",
    "\n",
    "        # feature block\n",
    "        x = torch.transpose(res, 3, 2)\n",
    "        x = nn.BatchNorm2d(num_features=num_channels)(x)\n",
    "        x = nn.Linear(in_features=num_channels, out_features=num_channels)(x)\n",
    "        x = nn.ReLU(x)\n",
    "        x = nn.Dropout(0.1)(x)\n",
    "        x = nn.Linear(in_features=num_channels, out_features=num_channels)(x)\n",
    "        x = nn.Dropout(0.1)(x)\n",
    "        return x + res\n",
    "\n",
    "    def forward(self, windows_batch):\n",
    "        inputs = windows_batch['insample_y']\n",
    "        batch_size = windows_batch.shape[0]\n",
    "        input_size = windows_batch.shape[1]\n",
    "        num_channels = windows_batch.shape[2]\n",
    "        horizon = self.h\n",
    "\n",
    "        layers = nn.ModuleList(\n",
    "            [# TODO loop this based on `num_stacks`\n",
    "             self.res_block(inputs=inputs,\n",
    "                            batch_size=batch_size,\n",
    "                            input_size=input_size,\n",
    "                            num_channels=num_channels),\n",
    "             torch.transpose(3, 2),\n",
    "             nn.Linear(in_features=input_size, out_features=horizon),\n",
    "             torch.transpose(3, 2)]\n",
    "            )\n",
    "        \n",
    "        for i, l in enumerate(layers):\n",
    "            if i == 0:\n",
    "                x = l(inputs)\n",
    "            else:\n",
    "                x = (l(x))\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "optimizer got an empty parameter list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mlist\u001b[39m(model\u001b[39m.\u001b[39mparameters())\n\u001b[1;32m     31\u001b[0m fcst \u001b[39m=\u001b[39m NeuralForecast(models\u001b[39m=\u001b[39m[model], freq\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mM\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m fcst\u001b[39m.\u001b[39;49mfit(df\u001b[39m=\u001b[39;49mY_train_df, static_df\u001b[39m=\u001b[39;49mAirPassengersStatic, val_size\u001b[39m=\u001b[39;49m\u001b[39m12\u001b[39;49m)\n\u001b[1;32m     33\u001b[0m \u001b[39m# forecasts = fcst.predict(futr_df=Y_test_df)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/neuralforecast/neuralforecast/core.py:347\u001b[0m, in \u001b[0;36mNeuralForecast.fit\u001b[0;34m(self, df, static_df, val_size, sort_df, use_init_models, verbose, id_col, time_col, target_col)\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mWARNING: Deleting previously fitted models.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    346\u001b[0m \u001b[39mfor\u001b[39;00m model \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodels:\n\u001b[0;32m--> 347\u001b[0m     model\u001b[39m.\u001b[39;49mfit(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset, val_size\u001b[39m=\u001b[39;49mval_size)\n\u001b[1;32m    349\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fitted \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/neuralforecast/neuralforecast/common/_base_multivariate.py:616\u001b[0m, in \u001b[0;36mBaseMultivariate.fit\u001b[0;34m(self, dataset, val_size, test_size, random_seed)\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer_kwargs[\u001b[39m\"\u001b[39m\u001b[39mcheck_val_every_n_epoch\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    615\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer_kwargs)\n\u001b[0;32m--> 616\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(\u001b[39mself\u001b[39;49m, datamodule\u001b[39m=\u001b[39;49mdatamodule)\n",
      "File \u001b[0;32m~/anaconda3/envs/neuralforecast/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m TrainerStatus\u001b[39m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    545\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    546\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/neuralforecast/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/anaconda3/envs/neuralforecast/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    574\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    575\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    576\u001b[0m     ckpt_path,\n\u001b[1;32m    577\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m )\n\u001b[0;32m--> 580\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    582\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    583\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/neuralforecast/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:965\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    962\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_logger_connector\u001b[39m.\u001b[39mreset_metrics()\n\u001b[1;32m    964\u001b[0m \u001b[39m# strategy will configure model and move it to the device\u001b[39;00m\n\u001b[0;32m--> 965\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstrategy\u001b[39m.\u001b[39;49msetup(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    967\u001b[0m \u001b[39m# hook\u001b[39;00m\n\u001b[1;32m    968\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39m==\u001b[39m TrainerFn\u001b[39m.\u001b[39mFITTING:\n",
      "File \u001b[0;32m~/anaconda3/envs/neuralforecast/lib/python3.10/site-packages/pytorch_lightning/strategies/single_device.py:78\u001b[0m, in \u001b[0;36mSingleDeviceStrategy.setup\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msetup\u001b[39m(\u001b[39mself\u001b[39m, trainer: pl\u001b[39m.\u001b[39mTrainer) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_to_device()\n\u001b[0;32m---> 78\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49msetup(trainer)\n",
      "File \u001b[0;32m~/anaconda3/envs/neuralforecast/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:151\u001b[0m, in \u001b[0;36mStrategy.setup\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39msetup(trainer)\n\u001b[0;32m--> 151\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msetup_optimizers(trainer)\n\u001b[1;32m    152\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msetup_precision_plugin()\n\u001b[1;32m    153\u001b[0m _optimizers_to_device(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/neuralforecast/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:140\u001b[0m, in \u001b[0;36mStrategy.setup_optimizers\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr_scheduler_configs \u001b[39m=\u001b[39m _init_optimizers_and_lr_schedulers(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlightning_module)\n",
      "File \u001b[0;32m~/anaconda3/envs/neuralforecast/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py:177\u001b[0m, in \u001b[0;36m_init_optimizers_and_lr_schedulers\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Calls `LightningModule.configure_optimizers` and parses and validates the output.\"\"\"\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrainer\u001b[39;00m \u001b[39mimport\u001b[39;00m call\n\u001b[0;32m--> 177\u001b[0m optim_conf \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39;49m_call_lightning_module_hook(model\u001b[39m.\u001b[39;49mtrainer, \u001b[39m\"\u001b[39;49m\u001b[39mconfigure_optimizers\u001b[39;49m\u001b[39m\"\u001b[39;49m, pl_module\u001b[39m=\u001b[39;49mmodel)\n\u001b[1;32m    179\u001b[0m \u001b[39mif\u001b[39;00m optim_conf \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m     rank_zero_warn(\n\u001b[1;32m    181\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    182\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/neuralforecast/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:157\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m hook_name\n\u001b[1;32m    156\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[LightningModule]\u001b[39m\u001b[39m{\u001b[39;00mpl_module\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 157\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    159\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    160\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/Documents/GitHub/neuralforecast/neuralforecast/common/_base_multivariate.py:160\u001b[0m, in \u001b[0;36mBaseMultivariate.configure_optimizers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconfigure_optimizers\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 160\u001b[0m     optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49moptim\u001b[39m.\u001b[39;49mAdam(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparameters(), lr\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearning_rate)\n\u001b[1;32m    161\u001b[0m     scheduler \u001b[39m=\u001b[39m {\n\u001b[1;32m    162\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mscheduler\u001b[39m\u001b[39m\"\u001b[39m: torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mStepLR(\n\u001b[1;32m    163\u001b[0m             optimizer\u001b[39m=\u001b[39moptimizer, step_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr_decay_steps, gamma\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minterval\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    167\u001b[0m     }\n\u001b[1;32m    168\u001b[0m     \u001b[39mreturn\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39moptimizer\u001b[39m\u001b[39m\"\u001b[39m: optimizer, \u001b[39m\"\u001b[39m\u001b[39mlr_scheduler\u001b[39m\u001b[39m\"\u001b[39m: scheduler}\n",
      "File \u001b[0;32m~/anaconda3/envs/neuralforecast/lib/python3.10/site-packages/torch/optim/adam.py:45\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid weight_decay value: \u001b[39m\u001b[39m{\u001b[39;00mweight_decay\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m defaults \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(lr\u001b[39m=\u001b[39mlr, betas\u001b[39m=\u001b[39mbetas, eps\u001b[39m=\u001b[39meps,\n\u001b[1;32m     42\u001b[0m                 weight_decay\u001b[39m=\u001b[39mweight_decay, amsgrad\u001b[39m=\u001b[39mamsgrad,\n\u001b[1;32m     43\u001b[0m                 maximize\u001b[39m=\u001b[39mmaximize, foreach\u001b[39m=\u001b[39mforeach, capturable\u001b[39m=\u001b[39mcapturable,\n\u001b[1;32m     44\u001b[0m                 differentiable\u001b[39m=\u001b[39mdifferentiable, fused\u001b[39m=\u001b[39mfused)\n\u001b[0;32m---> 45\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(params, defaults)\n\u001b[1;32m     47\u001b[0m \u001b[39mif\u001b[39;00m fused:\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m differentiable:\n",
      "File \u001b[0;32m~/anaconda3/envs/neuralforecast/lib/python3.10/site-packages/torch/optim/optimizer.py:261\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m    259\u001b[0m param_groups \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(params)\n\u001b[1;32m    260\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(param_groups) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 261\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39moptimizer got an empty parameter list\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    262\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(param_groups[\u001b[39m0\u001b[39m], \u001b[39mdict\u001b[39m):\n\u001b[1;32m    263\u001b[0m     param_groups \u001b[39m=\u001b[39m [{\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m: param_groups}]\n",
      "\u001b[0;31mValueError\u001b[0m: optimizer got an empty parameter list"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.utils import AirPassengersPanel, AirPassengersStatic\n",
    "from neuralforecast.losses.pytorch import MAE\n",
    "\n",
    "Y_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 132 train\n",
    "Y_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n",
    "\n",
    "model = TSMixer(h=12,\n",
    "                input_size=24,\n",
    "                n_series=2,\n",
    "                stat_exog_list=['airline1'],\n",
    "                futr_exog_list=['trend'],\n",
    "                scaler_type='robust',\n",
    "                max_steps=200,\n",
    "                early_stop_patience_steps=-1,\n",
    "                val_check_steps=10,\n",
    "                learning_rate=1e-3,\n",
    "                loss=None,\n",
    "                valid_loss=None,\n",
    "                batch_size=32\n",
    "                )\n",
    "\n",
    "list(model.parameters())\n",
    "\n",
    "fcst = NeuralForecast(models=[model], freq='M')\n",
    "fcst.fit(df=Y_train_df, static_df=AirPassengersStatic, val_size=12)\n",
    "# forecasts = fcst.predict(futr_df=Y_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "super(type, obj): obj must be an instance or subtype of type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 106\u001b[0m\n\u001b[1;32m    104\u001b[0m W \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mshape[\u001b[39m3\u001b[39m]\n\u001b[1;32m    105\u001b[0m T \u001b[39m=\u001b[39m \u001b[39m90\u001b[39m\n\u001b[0;32m--> 106\u001b[0m m \u001b[39m=\u001b[39m NeuralNetwork(inputs\u001b[39m=\u001b[39;49minputs, C\u001b[39m=\u001b[39;49mC, H\u001b[39m=\u001b[39;49mH, T\u001b[39m=\u001b[39;49mT, h\u001b[39m=\u001b[39;49m\u001b[39m12\u001b[39;49m,\n\u001b[1;32m    107\u001b[0m                  input_size\u001b[39m=\u001b[39;49m\u001b[39m24\u001b[39;49m,\n\u001b[1;32m    108\u001b[0m                  n_series\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,)\n\u001b[1;32m    109\u001b[0m \u001b[39mlist\u001b[39m(m\u001b[39m.\u001b[39mparameters())\n",
      "Cell \u001b[0;32mIn[34], line 36\u001b[0m, in \u001b[0;36mNeuralNetwork.__init__\u001b[0;34m(self, C, H, T, inputs, h, input_size, n_series, futr_exog_list, hist_exog_list, stat_exog_list, n_stacks, multi_layer, dropout_rate, leaky_rate, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, step_size, scaler_type, random_seed, num_workers_loader, drop_last_loader, **trainer_kwargs)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[1;32m      6\u001b[0m              C,\n\u001b[1;32m      7\u001b[0m              H,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m \n\u001b[1;32m     35\u001b[0m     \u001b[39m# Inherit BaseMultivariate class\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m     \u001b[39msuper\u001b[39;49m(TSMixer, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(h\u001b[39m=\u001b[39mh,\n\u001b[1;32m     37\u001b[0m                                   input_size\u001b[39m=\u001b[39minput_size,\n\u001b[1;32m     38\u001b[0m                                   n_series\u001b[39m=\u001b[39mn_series,\n\u001b[1;32m     39\u001b[0m                                   futr_exog_list\u001b[39m=\u001b[39mfutr_exog_list,\n\u001b[1;32m     40\u001b[0m                                   hist_exog_list\u001b[39m=\u001b[39mhist_exog_list,\n\u001b[1;32m     41\u001b[0m                                   stat_exog_list\u001b[39m=\u001b[39mstat_exog_list,                                    \n\u001b[1;32m     42\u001b[0m                                   loss\u001b[39m=\u001b[39mloss,\n\u001b[1;32m     43\u001b[0m                                   valid_loss\u001b[39m=\u001b[39mvalid_loss,\n\u001b[1;32m     44\u001b[0m                                   max_steps\u001b[39m=\u001b[39mmax_steps,\n\u001b[1;32m     45\u001b[0m                                   learning_rate\u001b[39m=\u001b[39mlearning_rate,\n\u001b[1;32m     46\u001b[0m                                   num_lr_decays\u001b[39m=\u001b[39mnum_lr_decays,\n\u001b[1;32m     47\u001b[0m                                   early_stop_patience_steps\u001b[39m=\u001b[39mearly_stop_patience_steps,\n\u001b[1;32m     48\u001b[0m                                   val_check_steps\u001b[39m=\u001b[39mval_check_steps,\n\u001b[1;32m     49\u001b[0m                                   batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m     50\u001b[0m                                   step_size\u001b[39m=\u001b[39mstep_size,\n\u001b[1;32m     51\u001b[0m                                   scaler_type\u001b[39m=\u001b[39mscaler_type,\n\u001b[1;32m     52\u001b[0m                                   num_workers_loader\u001b[39m=\u001b[39mnum_workers_loader,\n\u001b[1;32m     53\u001b[0m                                   drop_last_loader\u001b[39m=\u001b[39mdrop_last_loader,\n\u001b[1;32m     54\u001b[0m                                   random_seed\u001b[39m=\u001b[39mrandom_seed,\n\u001b[1;32m     55\u001b[0m                                   \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrainer_kwargs)\n\u001b[1;32m     57\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minputs \u001b[39m=\u001b[39m inputs\n\u001b[1;32m     58\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mC \u001b[39m=\u001b[39m C\n",
      "\u001b[0;31mTypeError\u001b[0m: super(type, obj): obj must be an instance or subtype of type"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(BaseMultivariate):\n",
    "        # Class attributes\n",
    "    SAMPLING_TYPE = 'multivariate'\n",
    "    \n",
    "    def __init__(self,\n",
    "                 C,\n",
    "                 H,\n",
    "                 T,\n",
    "                 inputs,\n",
    "                 h,\n",
    "                 input_size,\n",
    "                 n_series,\n",
    "                 futr_exog_list = None,\n",
    "                 hist_exog_list = None,\n",
    "                 stat_exog_list = None,\n",
    "                 n_stacks = 2,\n",
    "                 multi_layer: int = 5,\n",
    "                 dropout_rate: float = 0.5,\n",
    "                 leaky_rate: float = 0.2,\n",
    "                 loss = MAE(),\n",
    "                 valid_loss = None,\n",
    "                 max_steps: int = 1000,\n",
    "                 learning_rate: float = 1e-3,\n",
    "                 num_lr_decays: int = 3,\n",
    "                 early_stop_patience_steps: int =-1,\n",
    "                 val_check_steps: int = 100,\n",
    "                 batch_size: int = 32,\n",
    "                 step_size: int = 1,\n",
    "                 scaler_type: str = 'robust',\n",
    "                 random_seed: int = 1,\n",
    "                 num_workers_loader = 0,\n",
    "                 drop_last_loader = False,\n",
    "                 **trainer_kwargs):\n",
    "\n",
    "        # Inherit BaseMultivariate class\n",
    "        super(TSMixer, self).__init__(h=h,\n",
    "                                      input_size=input_size,\n",
    "                                      n_series=n_series,\n",
    "                                      futr_exog_list=futr_exog_list,\n",
    "                                      hist_exog_list=hist_exog_list,\n",
    "                                      stat_exog_list=stat_exog_list,                                    \n",
    "                                      loss=loss,\n",
    "                                      valid_loss=valid_loss,\n",
    "                                      max_steps=max_steps,\n",
    "                                      learning_rate=learning_rate,\n",
    "                                      num_lr_decays=num_lr_decays,\n",
    "                                      early_stop_patience_steps=early_stop_patience_steps,\n",
    "                                      val_check_steps=val_check_steps,\n",
    "                                      batch_size=batch_size,\n",
    "                                      step_size=step_size,\n",
    "                                      scaler_type=scaler_type,\n",
    "                                      num_workers_loader=num_workers_loader,\n",
    "                                      drop_last_loader=drop_last_loader,\n",
    "                                      random_seed=random_seed,\n",
    "                                      **trainer_kwargs)\n",
    "\n",
    "        self.inputs = inputs\n",
    "        self.C = C\n",
    "        self.H = H\n",
    "        self.T = T\n",
    "\n",
    "        self.batch_norm = nn.BatchNorm2d(num_features=C)\n",
    "        self.linear = nn.Linear(in_features=H, out_features=H)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        inputs = self.inputs\n",
    "        H = self.H\n",
    "        C = self.C\n",
    "        T = self.T\n",
    "\n",
    "        # time block\n",
    "        x = self.batch_norm()\n",
    "        x = torch.transpose(x, 3, 2)\n",
    "        x = self.linear(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        res = inputs + x\n",
    "\n",
    "        # feature block\n",
    "        x = torch.transpose(res, 3, 2)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(0.1)(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + res\n",
    "\n",
    "        # temporal projection\n",
    "        x = torch.transpose(res, 3, 2)\n",
    "        x = nn.Linear(in_features=H, out_features=T)(x)\n",
    "        x = torch.transpose(x, 3, 2)\n",
    "        return x\n",
    "\n",
    "shape = (1, 1, 365, 1) # (batch_size, channels, height, width) for unnivariate data\n",
    "shape = (1, 2, 365, 1) # (batch_size, channels, height, width) for multi variate data\n",
    "inputs = torch.rand(shape)\n",
    "\n",
    "N = inputs.shape[0]\n",
    "C = inputs.shape[1]\n",
    "H = inputs.shape[2]\n",
    "W = inputs.shape[3]\n",
    "T = 90\n",
    "m = NeuralNetwork(inputs=inputs, C=C, H=H, T=T, h=12,\n",
    "                 input_size=24,\n",
    "                 n_series=2,)\n",
    "list(m.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0076,  0.0512,  0.0308,  ...,  0.0056,  0.0078,  0.0049],\n",
       "         [ 0.0094,  0.0163, -0.0493,  ...,  0.0114,  0.0445,  0.0011],\n",
       "         [-0.0361, -0.0072,  0.0217,  ...,  0.0083,  0.0368, -0.0186],\n",
       "         ...,\n",
       "         [-0.0474, -0.0363,  0.0483,  ...,  0.0089,  0.0008,  0.0405],\n",
       "         [ 0.0172,  0.0100,  0.0167,  ..., -0.0243, -0.0012,  0.0497],\n",
       "         [ 0.0407, -0.0343,  0.0194,  ...,  0.0160, -0.0240, -0.0064]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 1.6142e-02, -2.8519e-02,  2.0091e-02, -3.0201e-02,  1.0083e-02,\n",
       "         -3.6561e-03, -3.5331e-02, -3.7638e-02, -6.0492e-03,  2.5079e-02,\n",
       "          5.0956e-02, -4.9123e-02,  4.1784e-02,  1.7217e-02,  2.8004e-02,\n",
       "         -1.9967e-02,  1.4762e-02, -1.1958e-02,  9.2816e-03,  3.1025e-02,\n",
       "          7.4955e-03,  4.2849e-02,  3.3125e-02, -2.2864e-02,  3.2638e-02,\n",
       "          1.6386e-02,  1.4108e-03,  2.8690e-02,  3.8633e-02, -3.7336e-03,\n",
       "          6.9335e-03,  4.4487e-02,  2.4317e-02, -1.5640e-02,  1.4667e-02,\n",
       "         -1.6200e-02, -1.4831e-03, -4.3138e-03,  3.9970e-02,  4.7329e-02,\n",
       "          4.4587e-03,  9.1618e-03,  4.6726e-02, -4.9618e-02, -4.0263e-02,\n",
       "          3.8148e-02, -5.1480e-02,  1.2559e-02,  4.3619e-02, -3.6418e-02,\n",
       "         -2.4991e-02,  1.1053e-02,  3.2876e-03, -1.8879e-03, -2.3208e-02,\n",
       "          3.0442e-02, -5.1795e-02, -2.1717e-02, -3.4317e-02,  8.4949e-03,\n",
       "         -2.4403e-02,  1.3556e-02,  4.2163e-02,  3.5648e-02, -9.6640e-05,\n",
       "         -2.1739e-02, -2.4608e-02,  4.4063e-02,  1.6427e-02,  1.6457e-02,\n",
       "          5.0616e-02,  4.7655e-02,  2.4325e-02, -1.2004e-02,  3.9938e-02,\n",
       "         -5.0149e-02,  2.3155e-02,  5.1488e-02,  2.7305e-02,  4.3647e-05,\n",
       "          4.9976e-02,  4.3550e-02,  1.1183e-02,  4.4252e-02, -2.7773e-02,\n",
       "          4.2523e-02, -4.7282e-02, -1.2896e-02,  4.6553e-02, -2.8801e-02,\n",
       "         -5.0548e-02, -3.4810e-02,  4.5066e-02,  3.4589e-02,  3.9630e-02,\n",
       "         -3.2968e-02,  3.3988e-03, -7.2923e-03,  3.5324e-02, -3.0251e-02,\n",
       "          3.6930e-03,  4.2310e-02,  1.8751e-02,  4.9074e-02, -3.7741e-02,\n",
       "          2.4935e-02,  4.5554e-02, -3.8792e-02, -2.7265e-02,  4.1265e-02,\n",
       "         -2.1919e-02, -4.6085e-02, -8.4432e-03,  2.2323e-02, -2.9794e-03,\n",
       "         -1.0478e-02, -3.1601e-02, -5.1205e-02,  2.2483e-02,  5.1733e-02,\n",
       "         -1.2514e-02,  3.0954e-02,  3.9921e-03,  2.9258e-02, -4.6384e-02,\n",
       "         -2.8724e-02,  9.5632e-03,  3.9073e-02,  1.9619e-02,  1.4848e-02,\n",
       "          1.1344e-02,  3.5884e-02, -2.9017e-03,  2.0227e-02, -3.6937e-02,\n",
       "         -6.6507e-03,  1.1846e-02, -2.5334e-02,  1.9023e-02,  3.6877e-02,\n",
       "          4.7267e-02, -2.5586e-02,  3.1286e-02,  4.1212e-02,  3.9695e-02,\n",
       "         -4.6847e-02, -5.0950e-02, -1.3671e-02, -4.1274e-02, -5.2011e-02,\n",
       "          2.7586e-02,  1.9174e-03,  5.2252e-02,  4.0902e-03,  4.2322e-02,\n",
       "         -3.6982e-02,  3.6326e-02, -2.8608e-02,  4.6977e-02, -4.1659e-02,\n",
       "          1.0942e-02,  3.5044e-02, -3.8515e-02,  2.2577e-02,  4.2415e-02,\n",
       "         -4.4955e-02, -1.8243e-03, -2.9206e-03, -4.9079e-02, -5.7663e-03,\n",
       "         -2.4470e-02, -2.0394e-02, -3.6072e-02,  3.1940e-02, -4.2612e-02,\n",
       "          1.5146e-02,  2.2630e-02,  9.2714e-03, -1.3610e-02, -1.5948e-02,\n",
       "          2.0446e-02, -2.8997e-02, -3.2905e-02, -4.6730e-02, -4.6060e-02,\n",
       "          2.5911e-02,  2.6755e-02,  5.1335e-02, -1.6353e-02,  3.8154e-02,\n",
       "          2.0779e-02,  1.6608e-03,  1.2738e-02,  4.7294e-02,  1.0369e-02,\n",
       "         -1.0674e-02,  2.2821e-02,  3.2811e-02,  5.5927e-03,  2.6158e-02,\n",
       "          5.0744e-03, -2.3972e-02, -1.5462e-02, -5.2313e-02,  3.4665e-02,\n",
       "         -4.3909e-02,  3.7832e-02,  4.5065e-02, -2.2729e-02, -4.2788e-03,\n",
       "          2.4442e-02, -1.0891e-03,  2.1830e-02,  4.4838e-02,  3.7626e-02,\n",
       "         -3.2268e-03,  6.8259e-03,  1.5635e-02,  2.1832e-02, -4.8225e-03,\n",
       "          2.5438e-02,  2.9841e-02, -2.9210e-02, -3.8016e-02,  4.4516e-02,\n",
       "         -2.8991e-02,  2.7644e-02, -3.9144e-02,  4.6854e-03, -5.0129e-02,\n",
       "         -3.4469e-02, -4.5310e-02,  3.5526e-02, -4.3645e-03, -3.0856e-02,\n",
       "         -3.3894e-02,  1.5357e-02,  8.2724e-03,  3.2705e-02,  4.6222e-02,\n",
       "         -3.4146e-02,  3.1294e-02,  1.5023e-02,  3.8282e-02,  4.2638e-02,\n",
       "         -1.6780e-02, -3.5342e-02,  2.7782e-03,  4.2030e-02,  1.5341e-02,\n",
       "         -2.0921e-02,  4.5251e-02, -4.3826e-02, -1.2055e-02, -2.2700e-02,\n",
       "         -2.1219e-02, -1.6805e-02,  1.8242e-02,  3.9748e-02, -4.0693e-03,\n",
       "         -3.0741e-02, -2.8323e-02, -1.6474e-02,  4.3135e-02, -3.6240e-02,\n",
       "          1.5556e-02, -3.6735e-02, -8.5388e-03, -1.7879e-02, -4.1200e-02,\n",
       "          2.9683e-02,  2.0636e-02,  3.4389e-02,  2.2186e-02, -1.4187e-02,\n",
       "         -4.6332e-02,  4.6926e-02,  1.2907e-02, -5.5541e-03,  1.6404e-02,\n",
       "         -3.2522e-02,  4.9810e-02,  3.2731e-02, -2.3979e-02,  1.3667e-02,\n",
       "         -4.8026e-02,  3.3228e-02, -4.3098e-02, -3.6300e-02,  4.4005e-02,\n",
       "          3.7526e-03, -2.0599e-03, -1.1622e-03,  3.9639e-02, -1.6721e-02,\n",
       "          5.0398e-02,  3.3775e-02,  1.4272e-02,  2.2409e-03,  2.9105e-02,\n",
       "         -1.3823e-02, -4.1440e-02, -8.4261e-03, -3.9808e-02,  3.5430e-02,\n",
       "         -4.2822e-02,  2.8786e-02, -3.7199e-02,  1.1307e-02,  9.0512e-03,\n",
       "          3.7512e-02, -4.3305e-02,  2.4253e-02, -3.0727e-02,  4.7872e-02,\n",
       "         -3.4437e-02, -3.7415e-02,  2.3605e-02,  7.0118e-03, -4.5059e-02,\n",
       "         -4.2686e-02,  5.7559e-04,  1.0121e-02, -2.4614e-02,  2.5614e-03,\n",
       "          4.9102e-02, -1.9379e-02, -2.0888e-03,  1.0338e-02, -1.0813e-02,\n",
       "          4.2976e-02,  4.0132e-02, -4.4460e-03,  3.8206e-02, -5.3887e-03,\n",
       "          3.0079e-02,  3.1079e-02, -8.8608e-03, -4.3622e-02, -1.4243e-03,\n",
       "         -1.6976e-02,  2.0861e-02,  5.0296e-02,  1.3874e-02, -9.4073e-03,\n",
       "         -4.6440e-02, -4.2785e-03,  4.5279e-02,  3.0414e-02,  3.5240e-02,\n",
       "         -1.8491e-02, -9.9439e-03,  2.0778e-02,  4.8999e-02, -3.5521e-02,\n",
       "         -3.0080e-02, -1.3039e-03, -4.4517e-02,  2.6012e-02, -4.8803e-02,\n",
       "         -5.2027e-02,  5.0885e-02, -3.2872e-02,  4.9432e-02, -8.8944e-03],\n",
       "        requires_grad=True)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, inputs, C, H, T):\n",
    "        super().__init__()\n",
    "        self.C = C\n",
    "        self.H = H\n",
    "        self.T = T\n",
    "        self.inputs = inputs\n",
    "\n",
    "        self.batch_norm = nn.BatchNorm2d(num_features=C)\n",
    "        self.linear = nn.Linear(in_features=H, out_features=H)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        H = self.H\n",
    "        C = self.C\n",
    "        T = self.T\n",
    "        inputs = self.inputs\n",
    "\n",
    "        # time block\n",
    "        x = self.batch_norm()\n",
    "        x = torch.transpose(x, 3, 2)\n",
    "        x = self.linear(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        res = inputs + x\n",
    "\n",
    "        # feature block\n",
    "        x = torch.transpose(res, 3, 2)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(0.1)(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + res\n",
    "\n",
    "        # temporal projection\n",
    "        x = torch.transpose(res, 3, 2)\n",
    "        x = nn.Linear(in_features=H, out_features=T)(x)\n",
    "        x = torch.transpose(x, 3, 2)\n",
    "        return x\n",
    "\n",
    "N = inputs.shape[0]\n",
    "C = inputs.shape[1]\n",
    "H = inputs.shape[2]\n",
    "W = inputs.shape[3]\n",
    "T = 90\n",
    "m = NeuralNetwork(inputs=inputs, C=C, H=H, T=T)\n",
    "list(m.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## take 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TSMixer(BaseMultivariate):\n",
    "    # Class attributes\n",
    "    SAMPLING_TYPE = 'multivariate'\n",
    "    \n",
    "    def __init__(self,\n",
    "                 h,\n",
    "                 input_size,\n",
    "                 n_series,\n",
    "                 futr_exog_list = None,\n",
    "                 hist_exog_list = None,\n",
    "                 stat_exog_list = None,\n",
    "                 n_stacks = 2,\n",
    "                 multi_layer: int = 5,\n",
    "                 dropout_rate: float = 0.5,\n",
    "                 leaky_rate: float = 0.2,\n",
    "                 loss = MAE(),\n",
    "                 valid_loss = None,\n",
    "                 max_steps: int = 1000,\n",
    "                 learning_rate: float = 1e-3,\n",
    "                 num_lr_decays: int = 3,\n",
    "                 early_stop_patience_steps: int =-1,\n",
    "                 val_check_steps: int = 100,\n",
    "                 batch_size: int = 32,\n",
    "                 step_size: int = 1,\n",
    "                 scaler_type: str = 'robust',\n",
    "                 random_seed: int = 1,\n",
    "                 num_workers_loader = 0,\n",
    "                 drop_last_loader = False,\n",
    "                 **trainer_kwargs):\n",
    "\n",
    "        # Inherit BaseMultivariate class\n",
    "        super(TSMixer, self).__init__(h=h,\n",
    "                                      input_size=input_size,\n",
    "                                      n_series=n_series,\n",
    "                                      futr_exog_list=futr_exog_list,\n",
    "                                      hist_exog_list=hist_exog_list,\n",
    "                                      stat_exog_list=stat_exog_list,                                    \n",
    "                                      loss=loss,\n",
    "                                      valid_loss=valid_loss,\n",
    "                                      max_steps=max_steps,\n",
    "                                      learning_rate=learning_rate,\n",
    "                                      num_lr_decays=num_lr_decays,\n",
    "                                      early_stop_patience_steps=early_stop_patience_steps,\n",
    "                                      val_check_steps=val_check_steps,\n",
    "                                      batch_size=batch_size,\n",
    "                                      step_size=step_size,\n",
    "                                      scaler_type=scaler_type,\n",
    "                                      num_workers_loader=num_workers_loader,\n",
    "                                      drop_last_loader=drop_last_loader,\n",
    "                                      random_seed=random_seed,\n",
    "                                      **trainer_kwargs)\n",
    "\n",
    "        # Exogenous variables\n",
    "        self.futr_input_size = len(self.futr_exog_list)\n",
    "        self.hist_input_size = len(self.hist_exog_list)\n",
    "        self.stat_input_size = len(self.stat_exog_list)\n",
    "\n",
    "        self.unit = n_series\n",
    "        self.stack_cnt = n_stacks\n",
    "        self.alpha = leaky_rate\n",
    "        self.time_step = input_size\n",
    "        self.horizon = h\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.input_size = input_size\n",
    "        self.num_channels = n_series\n",
    "        self.H = H\n",
    "\n",
    "        self.batch_norm = nn.BatchNorm2d(num_features=self.num_channels)\n",
    "        self.linear = nn.Linear(in_features=H, out_features=H)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, windows_batch, *args, **kwargs):\n",
    "        inputs = windows_batch['insample_y']\n",
    "        print('raw', inputs.shape)\n",
    "        \n",
    "        x = inputs.unsqueeze(1)#.permute(0, 1, 2, 3)\n",
    "        print('unsqueeze', x.shape)\n",
    "        \n",
    "        for _ in range(0, 1): # self.stack_cnt\n",
    "            # time block\n",
    "            # x = nn.BatchNorm2d(num_features=self.num_channels)(x)\n",
    "            x = torch.transpose(x, 3, 2)\n",
    "            print('transpose', x.shape)\n",
    "            \n",
    "            x = nn.Linear(in_features=self.input_size, out_features=self.input_size)(x)\n",
    "            print('linear', x.shape)\n",
    "            \n",
    "            x = nn.ReLU()(x)\n",
    "            print('relu', x.shape)\n",
    "\n",
    "            x = nn.Dropout(p=self.dropout_rate)(x)\n",
    "            print('dropout', x.shape)\n",
    "\n",
    "            res = inputs.unsqueeze(1) + x\n",
    "            print('res', res.shape)\n",
    "            print('\\n')\n",
    "            print('--------')\n",
    "\n",
    "            # feature block\n",
    "            x = torch.transpose(res, 3, 2)\n",
    "            print('feature transpose', x.shape)\n",
    "\n",
    "            x = nn.BatchNorm2d(num_features=self.num_channels)(x)\n",
    "            print('batch norm', x.shape)\n",
    "\n",
    "            x = nn.Linear(in_features=self.input_size, out_features=self.input_size)(x)\n",
    "            print('linear', x.shape)\n",
    "\n",
    "            x = nn.ReLU()(x)\n",
    "            print('relu', x.shape)\n",
    "\n",
    "            x = nn.Dropout(p=self.dropout_rate)(x)\n",
    "            print('dropout', x.shape)\n",
    "\n",
    "            x = nn.Linear(in_features=self.input_size, out_features=self.input_size)(x)\n",
    "            print('linear', x.shape)\n",
    "\n",
    "            x = nn.Dropout(p=self.dropout_rate)(x)\n",
    "            print('dropout', x.shape)\n",
    "\n",
    "            x = x + res\n",
    "            print('final x', x.shape)\n",
    "\n",
    "        # temporal projection\n",
    "        x = torch.transpose(res, 3, 2)\n",
    "        x = nn.Linear(in_features=self.input_size, out_features=self.horizon)(x)\n",
    "        x = torch.transpose(x, 3, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67bcf0cf8bf94e38bb545944e8d8379b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw torch.Size([1, 24, 2])\n",
      "unsqueeze torch.Size([1, 1, 24, 2])\n",
      "transpose torch.Size([1, 1, 2, 24])\n",
      "linear torch.Size([1, 1, 2, 24])\n",
      "relu torch.Size([1, 1, 2, 24])\n",
      "dropout torch.Size([1, 1, 2, 24])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (24) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[166], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mlist\u001b[39m(model\u001b[39m.\u001b[39mparameters())\n\u001b[1;32m     18\u001b[0m fcst \u001b[39m=\u001b[39m NeuralForecast(models\u001b[39m=\u001b[39m[model], freq\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mM\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m fcst\u001b[39m.\u001b[39;49mfit(df\u001b[39m=\u001b[39;49mY_train_df, static_df\u001b[39m=\u001b[39;49mAirPassengersStatic, val_size\u001b[39m=\u001b[39;49m\u001b[39m12\u001b[39;49m)\n\u001b[1;32m     20\u001b[0m forecasts \u001b[39m=\u001b[39m fcst\u001b[39m.\u001b[39mpredict(futr_df\u001b[39m=\u001b[39mY_test_df)\n",
      "File \u001b[0;32m~/Documents/GitHub/neuralforecast/neuralforecast/core.py:347\u001b[0m, in \u001b[0;36mNeuralForecast.fit\u001b[0;34m(self, df, static_df, val_size, sort_df, use_init_models, verbose, id_col, time_col, target_col)\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mWARNING: Deleting previously fitted models.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    346\u001b[0m \u001b[39mfor\u001b[39;00m model \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodels:\n\u001b[0;32m--> 347\u001b[0m     model\u001b[39m.\u001b[39;49mfit(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset, val_size\u001b[39m=\u001b[39;49mval_size)\n\u001b[1;32m    349\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fitted \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/neuralforecast/neuralforecast/common/_base_multivariate.py:616\u001b[0m, in \u001b[0;36mBaseMultivariate.fit\u001b[0;34m(self, dataset, val_size, test_size, random_seed)\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer_kwargs[\u001b[39m\"\u001b[39m\u001b[39mcheck_val_every_n_epoch\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    615\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer_kwargs)\n\u001b[0;32m--> 616\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(\u001b[39mself\u001b[39;49m, datamodule\u001b[39m=\u001b[39;49mdatamodule)\n",
      "File \u001b[0;32m~/anaconda3/envs/neuralforecast/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m TrainerStatus\u001b[39m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    545\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    546\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/neuralforecast/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/anaconda3/envs/neuralforecast/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    574\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    575\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    576\u001b[0m     ckpt_path,\n\u001b[1;32m    577\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m )\n\u001b[0;32m--> 580\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    582\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    583\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/neuralforecast/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:989\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    986\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    987\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    988\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 989\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m    991\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    992\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    994\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/neuralforecast/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1033\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[1;32m   1032\u001b[0m     \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1033\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sanity_check()\n\u001b[1;32m   1034\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1035\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/anaconda3/envs/neuralforecast/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1062\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1059\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_start\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1061\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1062\u001b[0m val_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1064\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1066\u001b[0m \u001b[39m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/neuralforecast/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:182\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     context_manager \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mno_grad\n\u001b[1;32m    181\u001b[0m \u001b[39mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 182\u001b[0m     \u001b[39mreturn\u001b[39;00m loop_run(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/neuralforecast/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:134\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mis_last_batch \u001b[39m=\u001b[39m data_fetcher\u001b[39m.\u001b[39mdone\n\u001b[1;32m    133\u001b[0m     \u001b[39m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n\u001b[1;32m    135\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     \u001b[39m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/neuralforecast/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:391\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    385\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest_step\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    386\u001b[0m step_args \u001b[39m=\u001b[39m (\n\u001b[1;32m    387\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[1;32m    388\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    389\u001b[0m     \u001b[39melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    390\u001b[0m )\n\u001b[0;32m--> 391\u001b[0m output \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39;49m_call_strategy_hook(trainer, hook_name, \u001b[39m*\u001b[39;49mstep_args)\n\u001b[1;32m    393\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[1;32m    395\u001b[0m \u001b[39mif\u001b[39;00m using_dataloader_iter:\n\u001b[1;32m    396\u001b[0m     \u001b[39m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/neuralforecast/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 309\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    311\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    312\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/anaconda3/envs/neuralforecast/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:403\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module:\n\u001b[1;32m    402\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_redirection(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module, \u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 403\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlightning_module\u001b[39m.\u001b[39;49mvalidation_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/GitHub/neuralforecast/neuralforecast/common/_base_multivariate.py:464\u001b[0m, in \u001b[0;36mBaseMultivariate.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    455\u001b[0m windows_batch \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[1;32m    456\u001b[0m     insample_y\u001b[39m=\u001b[39minsample_y,  \u001b[39m# [Ws, L]\u001b[39;00m\n\u001b[1;32m    457\u001b[0m     insample_mask\u001b[39m=\u001b[39minsample_mask,  \u001b[39m# [Ws, L]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    460\u001b[0m     stat_exog\u001b[39m=\u001b[39mstat_exog,\n\u001b[1;32m    461\u001b[0m )  \u001b[39m# [Ws, 1]\u001b[39;00m\n\u001b[1;32m    463\u001b[0m \u001b[39m# Model Predictions\u001b[39;00m\n\u001b[0;32m--> 464\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(windows_batch)\n\u001b[1;32m    465\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss\u001b[39m.\u001b[39mis_distribution_output:\n\u001b[1;32m    466\u001b[0m     outsample_y, y_loc, y_scale \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inv_normalization(\n\u001b[1;32m    467\u001b[0m         y_hat\u001b[39m=\u001b[39moutsample_y, temporal_cols\u001b[39m=\u001b[39mbatch[\u001b[39m\"\u001b[39m\u001b[39mtemporal_cols\u001b[39m\u001b[39m\"\u001b[39m], y_idx\u001b[39m=\u001b[39my_idx\n\u001b[1;32m    468\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/neuralforecast/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/neuralforecast/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[165], line 96\u001b[0m, in \u001b[0;36mTSMixer.forward\u001b[0;34m(self, windows_batch, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m x \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(p\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout_rate)(x)\n\u001b[1;32m     94\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mdropout\u001b[39m\u001b[39m'\u001b[39m, x\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> 96\u001b[0m res \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m1\u001b[39;49m) \u001b[39m+\u001b[39;49m x\n\u001b[1;32m     97\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mres\u001b[39m\u001b[39m'\u001b[39m, res\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     98\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (24) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "model = TSMixer(h=12,\n",
    "                input_size=24,\n",
    "                n_series=2,\n",
    "                stat_exog_list=['airline1'],\n",
    "                futr_exog_list=['trend'],\n",
    "                scaler_type='robust',\n",
    "                max_steps=200,\n",
    "                early_stop_patience_steps=-1,\n",
    "                val_check_steps=10,\n",
    "                learning_rate=1e-3,\n",
    "                loss=None,\n",
    "                valid_loss=None,\n",
    "                batch_size=32\n",
    "                )\n",
    "\n",
    "list(model.parameters())\n",
    "\n",
    "fcst = NeuralForecast(models=[model], freq='M')\n",
    "fcst.fit(df=Y_train_df, static_df=AirPassengersStatic, val_size=12)\n",
    "forecasts = fcst.predict(futr_df=Y_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 2.6970e-02, -2.3103e-02, -1.0147e-02,  ...,  3.1881e-02,\n",
       "          -7.6078e-03,  2.1505e-02],\n",
       "         [ 2.5764e-03, -4.8852e-02, -2.5679e-02,  ..., -2.0233e-02,\n",
       "           3.5104e-03, -2.0108e-02],\n",
       "         [ 4.9278e-02,  5.0543e-02, -6.0997e-03,  ..., -3.9860e-02,\n",
       "          -3.8550e-02,  4.2704e-02],\n",
       "         ...,\n",
       "         [ 1.2923e-02,  5.0932e-02,  1.9553e-02,  ..., -1.0130e-02,\n",
       "           4.5885e-02, -4.2174e-02],\n",
       "         [ 2.2950e-02, -9.2104e-03, -2.2901e-02,  ...,  2.8679e-03,\n",
       "          -3.2190e-02,  4.1641e-03],\n",
       "         [ 4.4899e-02,  8.1050e-03, -2.6598e-02,  ...,  1.6064e-02,\n",
       "          -3.6001e-02,  1.9062e-05]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-4.2905e-02, -2.3027e-02,  2.1914e-02,  2.3291e-02,  3.6993e-02,\n",
       "          4.3965e-02,  3.8235e-02, -8.7991e-03, -4.0697e-02,  4.9842e-02,\n",
       "          1.3931e-02, -3.2903e-02,  5.0081e-03, -4.4159e-02, -3.3998e-02,\n",
       "          1.4198e-02,  1.7350e-03,  4.9878e-02, -3.7395e-02,  3.6658e-02,\n",
       "         -1.8123e-02, -4.5948e-02,  1.4774e-02, -5.7220e-03,  8.3311e-03,\n",
       "          4.6665e-02,  3.2734e-02,  1.9790e-02,  2.5863e-02, -2.3712e-02,\n",
       "          4.6910e-03, -4.1882e-02, -1.4042e-02,  1.8745e-02, -2.8053e-02,\n",
       "          3.8645e-02, -3.6139e-02, -3.0537e-02, -4.7550e-02, -1.0530e-02,\n",
       "         -1.3171e-03,  1.0020e-02,  3.5086e-02,  3.8890e-02, -3.0760e-02,\n",
       "          4.4134e-02, -1.8259e-02, -2.4426e-02,  6.2206e-03, -3.9148e-02,\n",
       "          1.9063e-02, -2.5260e-02,  8.5203e-03,  2.4581e-02, -3.9516e-02,\n",
       "          2.1903e-02, -2.3252e-02,  1.1648e-02, -2.5196e-02,  5.1946e-02,\n",
       "         -4.3929e-03,  3.2207e-02,  1.2806e-02, -4.2975e-02, -4.7624e-02,\n",
       "         -2.4654e-02, -3.5085e-03,  4.0713e-02, -5.1858e-02,  1.3368e-02,\n",
       "         -4.5545e-02,  2.1756e-02,  2.2524e-02, -4.4666e-02, -2.8019e-02,\n",
       "          2.6291e-03, -1.4501e-02,  4.0836e-02,  4.9519e-02, -4.7476e-02,\n",
       "         -3.2314e-02,  3.1892e-02, -4.8158e-02, -5.3203e-03, -2.1341e-02,\n",
       "         -2.1773e-02,  8.3625e-03, -2.6151e-02, -5.0395e-02,  5.2278e-02,\n",
       "          1.4300e-02,  2.7007e-02,  4.2430e-03, -4.9623e-02, -1.1620e-02,\n",
       "         -7.6382e-03, -5.0896e-02, -3.8531e-02,  1.3647e-04, -2.6144e-02,\n",
       "         -3.4985e-02, -3.0807e-02, -3.1893e-02,  3.7959e-02,  3.9600e-02,\n",
       "         -3.9239e-02, -5.1594e-02, -2.5572e-02,  1.4302e-02,  4.7568e-02,\n",
       "          5.0181e-03,  3.7093e-02, -1.7082e-02, -6.8470e-03,  1.7760e-02,\n",
       "         -3.8446e-02, -2.2097e-02, -2.0594e-02,  2.3429e-02, -4.7964e-02,\n",
       "         -3.1146e-02,  4.0412e-02,  4.0527e-02, -1.7307e-02,  3.5249e-02,\n",
       "         -1.1058e-02,  5.1487e-02, -2.4451e-02,  2.8173e-02,  2.6460e-03,\n",
       "          2.7079e-02,  1.1190e-03,  3.9824e-02,  3.9337e-03,  4.1322e-02,\n",
       "         -5.5080e-04, -7.6281e-03,  4.4863e-02,  2.3543e-02,  3.6673e-02,\n",
       "          3.3138e-02,  2.5052e-02, -2.3201e-03, -2.8832e-02,  4.7403e-02,\n",
       "         -3.9819e-02,  1.7933e-02, -4.9873e-02, -1.2794e-02, -3.7894e-02,\n",
       "          2.1585e-03,  2.3007e-02,  2.8573e-02,  4.2911e-02, -1.3907e-02,\n",
       "         -2.3425e-02,  3.9917e-02, -4.2573e-02, -1.0514e-02,  4.3347e-02,\n",
       "         -4.0353e-02,  1.1456e-02, -4.9403e-02, -1.8243e-03,  4.1117e-02,\n",
       "         -8.3839e-03, -6.3500e-03,  6.0192e-03, -3.9271e-02,  1.8113e-02,\n",
       "          4.7530e-02,  3.2928e-03,  2.2081e-02,  1.8851e-02,  1.4161e-02,\n",
       "         -2.4150e-02,  3.6030e-02,  5.0955e-03, -1.6202e-02, -2.2471e-02,\n",
       "         -8.0393e-03, -2.5623e-02, -4.9882e-02,  3.2350e-02,  5.1797e-02,\n",
       "         -3.7569e-02,  2.1785e-02,  1.6919e-02,  6.9077e-03, -2.7559e-02,\n",
       "          2.1248e-02,  3.3037e-02,  3.0816e-02,  3.3941e-02,  3.0630e-02,\n",
       "          3.5586e-02, -5.2219e-04, -6.5728e-03, -3.7074e-02, -4.9898e-02,\n",
       "         -1.4496e-02,  3.7844e-02, -1.6071e-02,  3.5271e-02, -4.7684e-02,\n",
       "          4.2506e-02,  1.2255e-02,  3.5129e-02, -4.3909e-02,  4.1338e-02,\n",
       "          3.3710e-02, -1.4476e-06, -2.2995e-02,  2.1656e-02, -1.0263e-02,\n",
       "         -4.5226e-02, -1.6157e-02,  1.1792e-02, -5.0990e-02,  4.8416e-02,\n",
       "          5.1998e-02, -4.1488e-02,  3.9045e-02,  4.4785e-03,  2.8813e-02,\n",
       "          4.0206e-02, -3.4767e-02,  3.4761e-02,  4.9467e-02, -4.1869e-03,\n",
       "         -3.7051e-02,  2.3181e-02, -2.6365e-02, -3.8949e-02,  3.6932e-02,\n",
       "          2.6585e-02,  5.8427e-03, -4.5596e-02, -8.6060e-03,  3.9378e-02,\n",
       "         -1.1163e-02,  5.1049e-02, -3.5844e-02, -4.5555e-02, -4.1174e-03,\n",
       "         -1.9928e-02, -5.0861e-02,  3.6684e-02, -3.6183e-02,  1.1754e-02,\n",
       "          3.0731e-03,  1.1299e-02,  1.3256e-02, -4.6877e-02, -2.9556e-02,\n",
       "          2.9282e-02, -4.7876e-02,  2.8575e-02,  5.1139e-02,  2.8941e-02,\n",
       "          2.9899e-02, -2.4572e-02,  3.5836e-03,  2.7102e-02,  3.8555e-02,\n",
       "          3.9593e-02, -4.1312e-03,  3.4950e-02, -3.3911e-02,  2.7795e-02,\n",
       "          3.2437e-02,  4.4012e-02, -1.8020e-02,  6.7356e-03,  3.0363e-02,\n",
       "          1.9051e-02, -6.2617e-03,  2.6772e-02, -2.1990e-02,  1.6217e-03,\n",
       "          2.7587e-02, -7.2262e-03,  2.7819e-02, -4.5737e-02, -4.9036e-02,\n",
       "          1.6993e-02,  4.8465e-02,  2.4027e-02,  3.4282e-02, -3.5894e-02,\n",
       "         -2.1921e-02, -6.8026e-03,  1.7846e-02,  1.9430e-02,  4.4812e-02,\n",
       "          2.3544e-02,  7.6316e-03, -2.5061e-03,  2.0679e-02,  2.4468e-04,\n",
       "          3.5619e-02,  2.8772e-02,  6.0324e-03, -2.4555e-02,  8.0137e-04,\n",
       "         -4.0264e-02, -4.1263e-02, -1.0481e-02,  1.2956e-02, -3.2366e-02,\n",
       "          1.7801e-02, -1.0834e-02, -3.9854e-03, -2.4889e-02, -3.9105e-02,\n",
       "          5.0860e-02, -2.0726e-02, -4.6235e-02, -1.7318e-02, -1.6755e-02,\n",
       "          2.7916e-02,  2.5705e-02,  9.1257e-03,  2.1662e-02,  1.6177e-02,\n",
       "          6.0060e-03,  1.0645e-02, -7.7393e-03,  1.0902e-02,  3.4698e-02,\n",
       "          1.5363e-02, -4.8341e-02, -4.4030e-02,  2.5516e-02,  3.6431e-02,\n",
       "         -5.1783e-02,  1.4800e-02,  5.3845e-03, -3.8479e-02, -4.8647e-02,\n",
       "         -3.2072e-02, -3.0510e-02,  2.3101e-02, -4.9082e-02,  2.2905e-02,\n",
       "          4.1198e-02,  1.3501e-02,  3.2552e-02, -3.1106e-02,  6.8412e-05,\n",
       "          5.0906e-02, -3.1395e-02,  4.0515e-02, -3.1494e-02,  4.2198e-02,\n",
       "          4.1243e-02, -3.3633e-02, -4.3026e-02, -4.3325e-02, -1.7592e-02,\n",
       "          1.7284e-02,  3.4217e-02,  1.2582e-02, -4.3238e-02,  2.7627e-02],\n",
       "        requires_grad=True)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralforecast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
